<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Talking Object Detector</title>
  <style>
    body { margin: 0; text-align: center; background: #111; color: white; font-family: sans-serif; }
    video, canvas { width: 100%; max-width: 480px; margin-top: 10px; border-radius: 12px; }
    #status { margin: 10px; font-size: 18px; }
    button { margin: 5px; padding: 10px 20px; font-size: 16px; border: none; border-radius: 8px; cursor: pointer; }
    #startBtn { background: lime; color: black; }
    #toggleBtn { background: dodgerblue; color: white; }
    #talkBtn { background: orange; color: black; }
    #aiToggleBtn { background: purple; color: white; }
    #micIcon { font-size: 40px; margin-top: 10px; }
    .listening { color: red; text-shadow: 0 0 10px red; }
    #chatBox { margin-top: 15px; }
    #chatInput { padding: 8px; width: 60%; border-radius: 6px; border: none; }
    #chatSend { padding: 8px 15px; border: none; border-radius: 6px; background: dodgerblue; color: white; }
    #chatLog { margin: 10px auto; max-width: 480px; max-height: 200px; overflow-y: auto; text-align: left; background: #222; padding: 10px; border-radius: 8px; }
    #chatLog p { margin: 5px 0; }
  </style>
  <!-- TensorFlow.js -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
  <!-- COCO-SSD model -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
</head>
<body>
  <h1>Talking Object Detector ü§ñ</h1>
  <button id="startBtn" onclick="enableSpeech()">Start Talking üîä</button>
  <button id="toggleBtn" onclick="toggleMode()">Mode: Serious</button>
  <button id="talkBtn" onclick="toggleListening()">Start Conversation üé§</button>
  <button id="aiToggleBtn" onclick="toggleAI()">Mode: Canned Replies ü§ñ</button>
  <div id="micIcon">üéôÔ∏è</div>
  <video id="camera" autoplay playsinline muted></video>
  <canvas id="overlay"></canvas>
  <div id="status">Loading model...</div>

  <!-- Chat box -->
  <div id="chatBox">
    <input id="chatInput" type="text" placeholder="Type to object...">
    <button id="chatSend" onclick="sendChat()">Send</button>
  </div>
  <div id="chatLog"></div>

  <!-- Load Transformers.js for GPT -->
  <script type="module">
    import { pipeline } from "https://cdn.jsdelivr.net/npm/@xenova/transformers";

    let generator;
    (async () => {
      const statusDiv = document.getElementById("status");
      statusDiv.innerText = "Loading GPT model (first load may take ~10s)...";
      generator = await pipeline("text-generation", "Xenova/distilgpt2");
      statusDiv.innerText = "GPT ready üß†";
    })();

    // Hook into global scope so the rest of the code can call it
    window.generateAIResponse = async function(prompt) {
      if (!generator) return "(AI still loading...)";
      const output = await generator(prompt, { max_new_tokens: 30 });
      return output[0].generated_text;
    }
  </script>

  <script>
    const video = document.getElementById('camera');
    const canvas = document.getElementById('overlay');
    const ctx = canvas.getContext('2d');
    const statusDiv = document.getElementById('status');
    const micIcon = document.getElementById('micIcon');
    let model;

    // State
    let lastSpoken = "";
    let lastSpokenTime = 0;
    const cooldown = 5000;
    let speechEnabled = false;
    let funnyMode = false;
    let listening = false;
    let useAI = false;

    // Small demo dictionary
    const responses = {
      "person": {"hi":"Hi there, human!","hello":"Hello! Nice to meet you.","what are you":"I‚Äôm a person, just like you."},
      "dog": {"hi":"Woof! Hi friend!","hello":"Hello! Tail wagging.","what are you":"I‚Äôm a dog ‚Äî man‚Äôs best friend."},
      "cat": {"hi":"Meow! Hello human!","hello":"Purr... hello!","what are you":"I‚Äôm a cat ‚Äî I rule the house."},
      "default": {"hi":"Hello there!","hello":"Hi, nice to see you!","what are you":"I‚Äôm just an object."}
    };

    function toggleAI() {
      useAI = !useAI;
      document.getElementById("aiToggleBtn").innerText = useAI ? "Mode: Language Model üß†" : "Mode: Canned Replies ü§ñ";
    }

    async function getReply(obj, userSpeech) {
      if (useAI) {
        return await window.generateAIResponse(userSpeech);
      } else {
        return responses[obj]?.[userSpeech] || responses[obj]?.hi || responses["default"].hi;
      }
    }

    function enableSpeech() {
      const msg = new SpeechSynthesisUtterance("Speech enabled");
      msg.lang = "en-US";
      window.speechSynthesis.speak(msg);
      speechEnabled = true;
      statusDiv.innerText = "Speech ready ‚úÖ";
    }

    function toggleMode() {
      funnyMode = !funnyMode;
      document.getElementById('toggleBtn').innerText = funnyMode ? "Mode: Funny ü§™" : "Mode: Serious";
    }

    function speak(text) {
      if (!speechEnabled || !window.speechSynthesis) return;
      window.speechSynthesis.cancel();
      const msg = new SpeechSynthesisUtterance(text);
      msg.lang = "en-US";
      msg.rate = 0.9;
      window.speechSynthesis.speak(msg);
    }

    // Speech recognition
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    let recognition;
    if (SpeechRecognition) {
      recognition = new SpeechRecognition();
      recognition.continuous = false;
      recognition.lang = "en-US";
      recognition.onresult = async (event) => {
        const userSpeech = event.results[0][0].transcript.toLowerCase();
        statusDiv.innerText = `You said: "${userSpeech}"`;
        let obj = lastSpoken.includes("dog") ? "dog" : lastSpoken.includes("cat") ? "cat" : "default";
        let reply = await getReply(obj, userSpeech);
        logChat("You", userSpeech);
        logChat(obj, reply);
        speak(reply);
        if (listening) recognition.start();
      };
    }

    function toggleListening() {
      if (!recognition) {
        alert("Speech recognition not supported in this browser.");
        return;
      }
      listening = !listening;
      if (listening) {
        recognition.start();
        micIcon.classList.add("listening");
        statusDiv.innerText = "üé§ Listening continuously...";
        document.getElementById('talkBtn').innerText = "Stop Conversation üõë";
      } else {
        recognition.stop();
        micIcon.classList.remove("listening");
        statusDiv.innerText = "Stopped listening.";
        document.getElementById('talkBtn').innerText = "Start Conversation üé§";
      }
    }

    async function sendChat() {
      const input = document.getElementById("chatInput");
      const message = input.value.toLowerCase().trim();
      if (!message) return;
      input.value = "";
      logChat("You", message);
      let obj = "default";
      let reply = await getReply(obj, message);
      logChat(obj, reply);
      speak(reply);
    }

    function logChat(sender, text) {
      const log = document.getElementById("chatLog");
      log.innerHTML += `<p><b>${sender}:</b> ${text}</p>`;
      log.scrollTop = log.scrollHeight;
    }

    async function initCamera() {
      const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: "environment" } });
      video.srcObject = stream;
      return new Promise(resolve => video.onloadedmetadata = () => {
        video.play();
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        resolve();
      });
    }

    async function loadModel() {
      model = await cocoSsd.load();
      statusDiv.innerText = "Model ready ‚úÖ (tap Start Talking)";
    }

    async function detectLoop() {
      if (!model) return;
      const predictions = await model.detect(video);
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
      if (predictions.length > 0) {
        predictions.forEach(pred => {
          ctx.strokeStyle = "lime";
          ctx.lineWidth = 2;
          ctx.strokeRect(...pred.bbox);
          ctx.fillStyle = "lime";
          ctx.fillText(`${pred.class} (${Math.round(pred.score*100)}%)`, pred.bbox[0], pred.bbox[1] > 10 ? pred.bbox[1]-5 : 10);
        });
        const now = Date.now();
        const names = predictions.filter(p => p.score > 0.6).map(p => p.class);
        if (names.length > 0) {
          const objectsSeen = names.join(", ");
          if (objectsSeen !== lastSpoken || (now - lastSpokenTime) > cooldown) {
            speak(`I see ${objectsSeen}`);
            statusDiv.innerText = `Detected: ${objectsSeen}`;
            lastSpoken = objectsSeen;
            lastSpokenTime = now;
          }
        }
      } else {
        statusDiv.innerText = "No objects detected";
      }
      requestAnimationFrame(detectLoop);
    }

    (async () => {
      await initCamera();
      await loadModel();
      detectLoop();
    })();
  </script>
</body>
</html>
