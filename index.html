<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Talking Object Detector</title>
  <style>
    body { margin: 0; text-align: center; background: #111; color: white; font-family: sans-serif; }
    video, canvas { width: 100%; max-width: 480px; margin-top: 10px; border-radius: 12px; }
    #status { margin: 10px; font-size: 18px; }
    button { margin: 5px; padding: 10px 20px; font-size: 16px; border: none; border-radius: 8px; cursor: pointer; }
    #startBtn { background: lime; color: black; }
    #toggleBtn { background: dodgerblue; color: white; }
    #talkBtn { background: orange; color: black; }
  </style>
  <!-- TensorFlow.js -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
  <!-- COCO-SSD model -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
</head>
<body>
  <h1>Talking Object Detector 🤖</h1>
  <button id="startBtn" onclick="enableSpeech()">Start Talking 🔊</button>
  <button id="toggleBtn" onclick="toggleMode()">Mode: Serious</button>
  <button id="talkBtn" onclick="listenToUser()">Talk to Object 🎤</button>
  <video id="camera" autoplay playsinline muted></video>
  <canvas id="overlay"></canvas>
  <div id="status">Loading model...</div>

  <script>
    const video = document.getElementById('camera');
    const canvas = document.getElementById('overlay');
    const ctx = canvas.getContext('2d');
    const statusDiv = document.getElementById('status');
    let model;

    // State
    let lastSpoken = "";
    let lastSpokenTime = 0;
    const cooldown = 5000;
    let speechEnabled = false;
    let funnyMode = false;

    // Funny phrases dictionary
    const phrases = {
      "cat": ["Meow!", "I’m a cat!", "Pet me human!"],
      "dog": ["Woof woof!", "I’m a dog!", "Throw the ball!"],
      "person": ["Hello there!", "I see you!", "Looking good today!"],
      "book": ["I am a book!", "Read me!", "Knowledge is power!"],
      "chair": ["I’m a chair!", "Take a seat!", "Rest those legs!"]
    };

    // "Hi" responses for every object
    const hiResponses = {
      "cat": "Meow! Hello human!",
      "dog": "Woof! Hi friend!",
      "person": "Hi there! Nice to meet you!",
      "book": "Hello! I’m full of stories.",
      "chair": "Hi! Take a seat if you like.",
      "default": "Hello! I’m happy to see you."
    };

    // 1. Camera setup
    async function initCamera() {
      const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: "environment" } });
      video.srcObject = stream;
      return new Promise(resolve => video.onloadedmetadata = () => {
        video.play();
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        resolve();
      });
    }

    // 2. Load model
    async function loadModel() {
      try {
        model = await cocoSsd.load();
        statusDiv.innerText = "Model ready ✅ (tap Start Talking)";
      } catch (err) {
        statusDiv.innerText = "❌ Error loading model";
        console.error(err);
      }
    }

    // 3. Enable speech
    function enableSpeech() {
      const msg = new SpeechSynthesisUtterance("Speech enabled");
      msg.lang = "en-US";
      window.speechSynthesis.speak(msg);
      speechEnabled = true;
      statusDiv.innerText = "Speech ready ✅";
    }

    // 4. Toggle mode
    function toggleMode() {
      funnyMode = !funnyMode;
      document.getElementById('toggleBtn').innerText = funnyMode ? "Mode: Funny 🤪" : "Mode: Serious";
    }

    // 5. Speech function
    function speak(text) {
      if (!speechEnabled || !window.speechSynthesis) return;
      window.speechSynthesis.cancel();
      const msg = new SpeechSynthesisUtterance(text);
      msg.lang = "en-US";
      msg.rate = 0.9;
      window.speechSynthesis.speak(msg);
    }

    // 6. Speech recognition setup
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    let recognition;
    if (SpeechRecognition) {
      recognition = new SpeechRecognition();
      recognition.continuous = false;
      recognition.lang = "en-US";

      recognition.onresult = (event) => {
        const userSpeech = event.results[0][0].transcript.toLowerCase();
        statusDiv.innerText = `You said: "${userSpeech}"`;

        // Check if user said "hi" or "hello"
        if (userSpeech.includes("hi") || userSpeech.includes("hello")) {
          let response;
          if (lastSpoken.includes("cat")) {
            response = hiResponses["cat"];
          } else if (lastSpoken.includes("dog")) {
            response = hiResponses["dog"];
          } else if (lastSpoken.includes("person")) {
            response = hiResponses["person"];
          } else if (lastSpoken.includes("book")) {
            response = hiResponses["book"];
          } else if (lastSpoken.includes("chair")) {
            response = hiResponses["chair"];
          } else {
            response = hiResponses["default"];
          }
          speak(response);
        } else {
          speak("I heard you say " + userSpeech);
        }
      };
    }

    function listenToUser() {
      if (!recognition) {
        alert("Speech recognition not supported in this browser.");
        return;
      }
      recognition.start();
      statusDiv.innerText = "🎤 Listening...";
    }

    // 7. Detection loop
    async function detectLoop() {
      if (!model) return;

      const predictions = await model.detect(video);
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

      if (predictions.length > 0) {
        predictions.forEach(pred => {
          ctx.strokeStyle = "lime";
          ctx.lineWidth = 2;
          ctx.strokeRect(...pred.bbox);
          ctx.fillStyle = "lime";
          ctx.fillText(`${pred.class} (${Math.round(pred.score*100)}%)`, pred.bbox[0], pred.bbox[1] > 10 ? pred.bbox[1]-5 : 10);
        });

        const now = Date.now();
        const names = predictions.filter(p => p.score > 0.6).map(p => p.class);

        if (names.length > 0) {
          const objectsSeen = names.join(", ");
          if (objectsSeen !== lastSpoken || (now - lastSpokenTime) > cooldown) {
            let phrase;
            if (funnyMode) {
              phrase = names.map(obj => {
                if (phrases[obj]) {
                  return phrases[obj][Math.floor(Math.random() * phrases[obj].length)];
                } else {
                  return `I see a ${obj}`;
                }
              }).join(" And ");
            } else {
              phrase = names.length === 1 ? `I see a ${names[0]}` : `I see ${names.join(" and ")}`;
            }
            speak(phrase);
            statusDiv.innerText = `Detected: ${objectsSeen}`;
            lastSpoken = objectsSeen;
            lastSpokenTime = now;
          }
        }
      } else {
        statusDiv.innerText = "No objects detected";
      }

      requestAnimationFrame(detectLoop);
    }

    // 8. Init
    (async () => {
      await initCamera();
      await loadModel();
      detectLoop();
    })();
  </script>
</body>
</html>
